{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92017fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import networkx.convert_matrix\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.utils.extmath import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4517691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre(train, content):\n",
    "    \n",
    "    \n",
    "    content.rename(columns={ content.columns[0]: \"id\" }, inplace=True)\n",
    "    contentArray = content.values\n",
    "    contentArray_sort = contentArray[contentArray[:,0].argsort()]\n",
    "    contentArray_sort = pd.DataFrame(contentArray_sort)\n",
    "\n",
    "    unconnected_pairs = []\n",
    "    matrix =np.zeros( (contentArray_sort.shape[0], contentArray_sort.shape[0]) )\n",
    "    \n",
    "    for index,row in train.iterrows():\n",
    "        if row[3]==1:\n",
    "            x = row[1]\n",
    "            y = row[2]\n",
    "            matrix[x, y] = 1\n",
    "            matrix[y, x] = 1\n",
    "        else:\n",
    "            x = row[1]\n",
    "            y = row[2]\n",
    "            matrix[x, y] = 0\n",
    "            matrix[y, x] = 0\n",
    "\n",
    "    return np.asmatrix(matrix), contentArray_sort\n",
    "def edge_sort_train(train):\n",
    "    train_link = train.loc[:,['to', 'from', 'label']]\n",
    "\n",
    "    for i in train_link.index:\n",
    "        t = train_link.loc[i]['to']\n",
    "        f = train_link.loc[i]['from']\n",
    "        if t > f :\n",
    "            train_link.loc[i]['to'] = f\n",
    "            train_link.loc[i]['from'] = t\n",
    "\n",
    "    train_link = train_link.sort_values(by='to')\n",
    "    train_link = train_link[~train_link.duplicated()] # -> with duplicated edge exists\n",
    "    train_link.index = range(train_link.shape[0])\n",
    "\n",
    "    return train_link\n",
    "\n",
    "def edge_sort_test(train):\n",
    "    train_link = train.loc[:,['id', 'to', 'from']]\n",
    "\n",
    "    for i in train_link.index:\n",
    "        t = train_link.loc[i]['to']\n",
    "        f = train_link.loc[i]['from']\n",
    "        if t > f :\n",
    "            train_link.loc[i]['to'] = f\n",
    "            train_link.loc[i]['from'] = t\n",
    "\n",
    "    train_link = train_link.sort_values(by='to')\n",
    "    train_link.index = range(train_link.shape[0])\n",
    "\n",
    "    return train_link\n",
    "\n",
    "def get_fea1(e_sort, str_fea):\n",
    "    \n",
    "    fea = []\n",
    "    for i in e_sort.index:\n",
    "\n",
    "        t = str_fea.loc[e_sort.loc[i, 'to'], :].values\n",
    "        f = str_fea.loc[e_sort.loc[i, 'from'], :].values\n",
    "        fea.append(t * f)\n",
    "\n",
    "    fea = np.array(fea)\n",
    "    \n",
    "    return fea\n",
    "\n",
    "def get_fea2(e_sort, str_fea):\n",
    "    \n",
    "    fea = []\n",
    "    for i in e_sort.index:\n",
    "\n",
    "        t = str_fea.loc[e_sort.loc[i, 'to'], :].values\n",
    "        f = str_fea.loc[e_sort.loc[i, 'from'], :].values\n",
    "        fea.append(np.dot(t, f))\n",
    "\n",
    "    fea = np.array(fea)\n",
    "    \n",
    "    return fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00df34ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-e247fccb91f9>:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_link.loc[i]['to'] = f\n",
      "<ipython-input-2-e247fccb91f9>:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_link.loc[i]['from'] = t\n"
     ]
    }
   ],
   "source": [
    "edge_test = pd.read_csv('test_3.csv')\n",
    "e_sort_test = edge_sort_test(edge_test)\n",
    "u_ex = pd.read_csv('upload_3.csv')\n",
    "train = pd.read_csv('train_3.csv')\n",
    "edge_train = edge_sort_train(train)\n",
    "content = pd.read_csv('content_3.csv', delimiter='\\t',header=None)\n",
    "content = content.drop(0, axis=1)\n",
    "train_adj, content_p = pre(train, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db83f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fea = pd.read_csv('n_100fea3.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "214594f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_c = np.concatenate((edge_train.loc[:, ['from', 'to']].values, edge_train.loc[:, ['to', 'from']].values), axis=0)\n",
    "edge_index = torch.tensor(edge_c, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96444da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(edge_index=edge_index.t())\n",
    "data.num_nodes= content.shape[0]\n",
    "data.x = torch.tensor(n_fea.values, dtype=torch.float32)\n",
    "data = train_test_split_edges(data,  test_ratio=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "000fa99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(data.x.shape[1], 70)\n",
    "        self.conv2 = GCNConv(70, 40)\n",
    "        self.conv3 = GCNConv(40, 16)\n",
    "\n",
    "    def encode(self):\n",
    "        x = self.conv1(data.x, data.train_pos_edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, data.train_pos_edge_index)\n",
    "        x = x.relu()\n",
    "        return self.conv3(x, data.train_pos_edge_index)\n",
    "\n",
    "    def decode(self, z, pos_edge_index, neg_edge_index):\n",
    "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1)\n",
    "        logits = (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n",
    "        return logits\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, data = Net().to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "def get_link_labels(pos_edge_index, neg_edge_index):\n",
    "    E = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
    "    link_labels = torch.zeros(E, dtype=torch.float, device=device)\n",
    "    link_labels[:pos_edge_index.size(1)] = 1.\n",
    "    return link_labels\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data.train_pos_edge_index, num_nodes=data.num_nodes,\n",
    "        num_neg_samples=data.train_pos_edge_index.size(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode()\n",
    "    link_logits = model.decode(z, data.train_pos_edge_index, neg_edge_index)\n",
    "    link_labels = get_link_labels(data.train_pos_edge_index, neg_edge_index)\n",
    "    loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    perfs = []\n",
    "    for prefix in [\"val\", \"test\"]:\n",
    "        pos_edge_index = data[f'{prefix}_pos_edge_index']\n",
    "        neg_edge_index = data[f'{prefix}_neg_edge_index']\n",
    "\n",
    "        z = model.encode()\n",
    "        link_logits = model.decode(z, pos_edge_index, neg_edge_index)\n",
    "        link_probs = link_logits.sigmoid()\n",
    "        link_labels = get_link_labels(pos_edge_index, neg_edge_index)\n",
    "        perfs.append(roc_auc_score(link_labels.cpu(), link_probs.cpu()))\n",
    "    return perfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d4c9f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6884, Val: 0.6636, Test: 0.5000\n",
      "Epoch: 002, Loss: 0.6756, Val: 0.6834, Test: 0.5000\n",
      "Epoch: 003, Loss: 0.6728, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 004, Loss: 0.6628, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 005, Loss: 0.6556, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 006, Loss: 0.6432, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 007, Loss: 0.6280, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 008, Loss: 0.6109, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 009, Loss: 0.5995, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 010, Loss: 0.5942, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 011, Loss: 0.5954, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 012, Loss: 0.5934, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 013, Loss: 0.5814, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 014, Loss: 0.5795, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 015, Loss: 0.5790, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 016, Loss: 0.5806, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 017, Loss: 0.5691, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 018, Loss: 0.5634, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 019, Loss: 0.5646, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 020, Loss: 0.5586, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 021, Loss: 0.5538, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 022, Loss: 0.5606, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 023, Loss: 0.5564, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 024, Loss: 0.5602, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 025, Loss: 0.5490, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 026, Loss: 0.5431, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 027, Loss: 0.5486, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 028, Loss: 0.5406, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 029, Loss: 0.5447, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 030, Loss: 0.5399, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 031, Loss: 0.5379, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 032, Loss: 0.5368, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 033, Loss: 0.5349, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 034, Loss: 0.5389, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 035, Loss: 0.5374, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 036, Loss: 0.5361, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 037, Loss: 0.5289, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 038, Loss: 0.5309, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 039, Loss: 0.5317, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 040, Loss: 0.5344, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 041, Loss: 0.5241, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 042, Loss: 0.5289, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 043, Loss: 0.5334, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 044, Loss: 0.5168, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 045, Loss: 0.5210, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 046, Loss: 0.5100, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 047, Loss: 0.5153, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 048, Loss: 0.5189, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 049, Loss: 0.5160, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 050, Loss: 0.5126, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 051, Loss: 0.5211, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 052, Loss: 0.5136, Val: 0.6859, Test: 0.5000\n",
      "Epoch: 053, Loss: 0.5225, Val: 0.6881, Test: 1.0000\n",
      "Epoch: 054, Loss: 0.5080, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 055, Loss: 0.5127, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 056, Loss: 0.5130, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 057, Loss: 0.5171, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 058, Loss: 0.5121, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 059, Loss: 0.5172, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 060, Loss: 0.5077, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 061, Loss: 0.5081, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 062, Loss: 0.5124, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 063, Loss: 0.5020, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 064, Loss: 0.5127, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 065, Loss: 0.5064, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 066, Loss: 0.5193, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 067, Loss: 0.5102, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 068, Loss: 0.5043, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 069, Loss: 0.5082, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 070, Loss: 0.5063, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 071, Loss: 0.5111, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 072, Loss: 0.5027, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 073, Loss: 0.5047, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 074, Loss: 0.5036, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 075, Loss: 0.5093, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 076, Loss: 0.5024, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 077, Loss: 0.5037, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 078, Loss: 0.5018, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 079, Loss: 0.5027, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 080, Loss: 0.5040, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 081, Loss: 0.5007, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 082, Loss: 0.5013, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 083, Loss: 0.5101, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 084, Loss: 0.5019, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 085, Loss: 0.5007, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 086, Loss: 0.4990, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 087, Loss: 0.4965, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 088, Loss: 0.4953, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 089, Loss: 0.5016, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 090, Loss: 0.5028, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 091, Loss: 0.5027, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 092, Loss: 0.4953, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 093, Loss: 0.4926, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 094, Loss: 0.4933, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 095, Loss: 0.5013, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 096, Loss: 0.4991, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 097, Loss: 0.4989, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 098, Loss: 0.4932, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 099, Loss: 0.4982, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 100, Loss: 0.4953, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 101, Loss: 0.4953, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 102, Loss: 0.4980, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 103, Loss: 0.5026, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 104, Loss: 0.4891, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 105, Loss: 0.5029, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 106, Loss: 0.4956, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 107, Loss: 0.4984, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 108, Loss: 0.4952, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 109, Loss: 0.4889, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 110, Loss: 0.4923, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 111, Loss: 0.4946, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 112, Loss: 0.4887, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 113, Loss: 0.5000, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 114, Loss: 0.4902, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 115, Loss: 0.4903, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 116, Loss: 0.4962, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 117, Loss: 0.4824, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 118, Loss: 0.4901, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 119, Loss: 0.4939, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 120, Loss: 0.4922, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 121, Loss: 0.4902, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 122, Loss: 0.4959, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 123, Loss: 0.4926, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 124, Loss: 0.4846, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 125, Loss: 0.4946, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 126, Loss: 0.4842, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 127, Loss: 0.4847, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 128, Loss: 0.4850, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 129, Loss: 0.4894, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 130, Loss: 0.4860, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 131, Loss: 0.4903, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 132, Loss: 0.4936, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 133, Loss: 0.4865, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 134, Loss: 0.4830, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 135, Loss: 0.4885, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 136, Loss: 0.4879, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 137, Loss: 0.4865, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 138, Loss: 0.4940, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 139, Loss: 0.4808, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 140, Loss: 0.4859, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 141, Loss: 0.4874, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 142, Loss: 0.4859, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 143, Loss: 0.4891, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 144, Loss: 0.4780, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 145, Loss: 0.4842, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 146, Loss: 0.4773, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 147, Loss: 0.4920, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 148, Loss: 0.4845, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 149, Loss: 0.4811, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 150, Loss: 0.4849, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 151, Loss: 0.4784, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 152, Loss: 0.4897, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 153, Loss: 0.4863, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 154, Loss: 0.4828, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 155, Loss: 0.4859, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 156, Loss: 0.4785, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 157, Loss: 0.4819, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 158, Loss: 0.4857, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 159, Loss: 0.4852, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 160, Loss: 0.4852, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 161, Loss: 0.4802, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 162, Loss: 0.4778, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 163, Loss: 0.4778, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 164, Loss: 0.4746, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 165, Loss: 0.4780, Val: 0.6897, Test: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 166, Loss: 0.4799, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 167, Loss: 0.4764, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 168, Loss: 0.4777, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 169, Loss: 0.4831, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 170, Loss: 0.4798, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 171, Loss: 0.4841, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 172, Loss: 0.4783, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 173, Loss: 0.4851, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 174, Loss: 0.4808, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 175, Loss: 0.4810, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 176, Loss: 0.4815, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 177, Loss: 0.4729, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 178, Loss: 0.4705, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 179, Loss: 0.4810, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 180, Loss: 0.4713, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 181, Loss: 0.4754, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 182, Loss: 0.4804, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 183, Loss: 0.4810, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 184, Loss: 0.4759, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 185, Loss: 0.4714, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 186, Loss: 0.4806, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 187, Loss: 0.4717, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 188, Loss: 0.4720, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 189, Loss: 0.4688, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 190, Loss: 0.4789, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 191, Loss: 0.4795, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 192, Loss: 0.4718, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 193, Loss: 0.4724, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 194, Loss: 0.4722, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 195, Loss: 0.4767, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 196, Loss: 0.4735, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 197, Loss: 0.4789, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 198, Loss: 0.4712, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 199, Loss: 0.4734, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 200, Loss: 0.4721, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 201, Loss: 0.4703, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 202, Loss: 0.4718, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 203, Loss: 0.4690, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 204, Loss: 0.4705, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 205, Loss: 0.4710, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 206, Loss: 0.4777, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 207, Loss: 0.4750, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 208, Loss: 0.4681, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 209, Loss: 0.4737, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 210, Loss: 0.4739, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 211, Loss: 0.4754, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 212, Loss: 0.4726, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 213, Loss: 0.4734, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 214, Loss: 0.4718, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 215, Loss: 0.4728, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 216, Loss: 0.4666, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 217, Loss: 0.4730, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 218, Loss: 0.4697, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 219, Loss: 0.4707, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 220, Loss: 0.4742, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 221, Loss: 0.4785, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 222, Loss: 0.4687, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 223, Loss: 0.4711, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 224, Loss: 0.4738, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 225, Loss: 0.4746, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 226, Loss: 0.4734, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 227, Loss: 0.4680, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 228, Loss: 0.4721, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 229, Loss: 0.4683, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 230, Loss: 0.4803, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 231, Loss: 0.4729, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 232, Loss: 0.4684, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 233, Loss: 0.4832, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 234, Loss: 0.4717, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 235, Loss: 0.4741, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 236, Loss: 0.4731, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 237, Loss: 0.4684, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 238, Loss: 0.4720, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 239, Loss: 0.4656, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 240, Loss: 0.4743, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 241, Loss: 0.4684, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 242, Loss: 0.4698, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 243, Loss: 0.4709, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 244, Loss: 0.4716, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 245, Loss: 0.4700, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 246, Loss: 0.4619, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 247, Loss: 0.4625, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 248, Loss: 0.4732, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 249, Loss: 0.4707, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 250, Loss: 0.4565, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 251, Loss: 0.4740, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 252, Loss: 0.4652, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 253, Loss: 0.4730, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 254, Loss: 0.4668, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 255, Loss: 0.4713, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 256, Loss: 0.4698, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 257, Loss: 0.4724, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 258, Loss: 0.4660, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 259, Loss: 0.4655, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 260, Loss: 0.4666, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 261, Loss: 0.4703, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 262, Loss: 0.4631, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 263, Loss: 0.4722, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 264, Loss: 0.4646, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 265, Loss: 0.4665, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 266, Loss: 0.4758, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 267, Loss: 0.4642, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 268, Loss: 0.4633, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 269, Loss: 0.4671, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 270, Loss: 0.4696, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 271, Loss: 0.4647, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 272, Loss: 0.4730, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 273, Loss: 0.4692, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 274, Loss: 0.4605, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 275, Loss: 0.4630, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 276, Loss: 0.4637, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 277, Loss: 0.4677, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 278, Loss: 0.4644, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 279, Loss: 0.4566, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 280, Loss: 0.4645, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 281, Loss: 0.4669, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 282, Loss: 0.4595, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 283, Loss: 0.4663, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 284, Loss: 0.4618, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 285, Loss: 0.4658, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 286, Loss: 0.4626, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 287, Loss: 0.4592, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 288, Loss: 0.4613, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 289, Loss: 0.4661, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 290, Loss: 0.4583, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 291, Loss: 0.4674, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 292, Loss: 0.4642, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 293, Loss: 0.4654, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 294, Loss: 0.4661, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 295, Loss: 0.4616, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 296, Loss: 0.4671, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 297, Loss: 0.4666, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 298, Loss: 0.4649, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 299, Loss: 0.4605, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 300, Loss: 0.4618, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 301, Loss: 0.4621, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 302, Loss: 0.4617, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 303, Loss: 0.4722, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 304, Loss: 0.4623, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 305, Loss: 0.4580, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 306, Loss: 0.4692, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 307, Loss: 0.4539, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 308, Loss: 0.4647, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 309, Loss: 0.4587, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 310, Loss: 0.4706, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 311, Loss: 0.4634, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 312, Loss: 0.4686, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 313, Loss: 0.4613, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 314, Loss: 0.4639, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 315, Loss: 0.4607, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 316, Loss: 0.4607, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 317, Loss: 0.4575, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 318, Loss: 0.4632, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 319, Loss: 0.4636, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 320, Loss: 0.4588, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 321, Loss: 0.4650, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 322, Loss: 0.4588, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 323, Loss: 0.4619, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 324, Loss: 0.4638, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 325, Loss: 0.4684, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 326, Loss: 0.4597, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 327, Loss: 0.4624, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 328, Loss: 0.4646, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 329, Loss: 0.4599, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 330, Loss: 0.4634, Val: 0.6897, Test: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 331, Loss: 0.4624, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 332, Loss: 0.4702, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 333, Loss: 0.4663, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 334, Loss: 0.4652, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 335, Loss: 0.4629, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 336, Loss: 0.4591, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 337, Loss: 0.4608, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 338, Loss: 0.4631, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 339, Loss: 0.4618, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 340, Loss: 0.4611, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 341, Loss: 0.4600, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 342, Loss: 0.4604, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 343, Loss: 0.4543, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 344, Loss: 0.4573, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 345, Loss: 0.4632, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 346, Loss: 0.4579, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 347, Loss: 0.4547, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 348, Loss: 0.4537, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 349, Loss: 0.4622, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 350, Loss: 0.4592, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 351, Loss: 0.4616, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 352, Loss: 0.4642, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 353, Loss: 0.4543, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 354, Loss: 0.4578, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 355, Loss: 0.4578, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 356, Loss: 0.4554, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 357, Loss: 0.4588, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 358, Loss: 0.4632, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 359, Loss: 0.4609, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 360, Loss: 0.4525, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 361, Loss: 0.4552, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 362, Loss: 0.4582, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 363, Loss: 0.4604, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 364, Loss: 0.4550, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 365, Loss: 0.4625, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 366, Loss: 0.4593, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 367, Loss: 0.4517, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 368, Loss: 0.4645, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 369, Loss: 0.4553, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 370, Loss: 0.4556, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 371, Loss: 0.4542, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 372, Loss: 0.4545, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 373, Loss: 0.4519, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 374, Loss: 0.4513, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 375, Loss: 0.4534, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 376, Loss: 0.4525, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 377, Loss: 0.4505, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 378, Loss: 0.4605, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 379, Loss: 0.4536, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 380, Loss: 0.4622, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 381, Loss: 0.4498, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 382, Loss: 0.4520, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 383, Loss: 0.4561, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 384, Loss: 0.4579, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 385, Loss: 0.4619, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 386, Loss: 0.4625, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 387, Loss: 0.4443, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 388, Loss: 0.4557, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 389, Loss: 0.4584, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 390, Loss: 0.4535, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 391, Loss: 0.4635, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 392, Loss: 0.4521, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 393, Loss: 0.4589, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 394, Loss: 0.4621, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 395, Loss: 0.4565, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 396, Loss: 0.4577, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 397, Loss: 0.4527, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 398, Loss: 0.4558, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 399, Loss: 0.4598, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 400, Loss: 0.4485, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 401, Loss: 0.4508, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 402, Loss: 0.4560, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 403, Loss: 0.4564, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 404, Loss: 0.4525, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 405, Loss: 0.4484, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 406, Loss: 0.4583, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 407, Loss: 0.4542, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 408, Loss: 0.4600, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 409, Loss: 0.4529, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 410, Loss: 0.4532, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 411, Loss: 0.4551, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 412, Loss: 0.4575, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 413, Loss: 0.4507, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 414, Loss: 0.4505, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 415, Loss: 0.4598, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 416, Loss: 0.4623, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 417, Loss: 0.4530, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 418, Loss: 0.4565, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 419, Loss: 0.4532, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 420, Loss: 0.4547, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 421, Loss: 0.4537, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 422, Loss: 0.4535, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 423, Loss: 0.4570, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 424, Loss: 0.4546, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 425, Loss: 0.4516, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 426, Loss: 0.4596, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 427, Loss: 0.4604, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 428, Loss: 0.4529, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 429, Loss: 0.4567, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 430, Loss: 0.4567, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 431, Loss: 0.4440, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 432, Loss: 0.4473, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 433, Loss: 0.4529, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 434, Loss: 0.4512, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 435, Loss: 0.4578, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 436, Loss: 0.4557, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 437, Loss: 0.4495, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 438, Loss: 0.4444, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 439, Loss: 0.4548, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 440, Loss: 0.4506, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 441, Loss: 0.4482, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 442, Loss: 0.4455, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 443, Loss: 0.4482, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 444, Loss: 0.4512, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 445, Loss: 0.4478, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 446, Loss: 0.4521, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 447, Loss: 0.4432, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 448, Loss: 0.4462, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 449, Loss: 0.4580, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 450, Loss: 0.4523, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 451, Loss: 0.4488, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 452, Loss: 0.4482, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 453, Loss: 0.4470, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 454, Loss: 0.4512, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 455, Loss: 0.4492, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 456, Loss: 0.4501, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 457, Loss: 0.4553, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 458, Loss: 0.4452, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 459, Loss: 0.4478, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 460, Loss: 0.4466, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 461, Loss: 0.4515, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 462, Loss: 0.4505, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 463, Loss: 0.4461, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 464, Loss: 0.4515, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 465, Loss: 0.4511, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 466, Loss: 0.4408, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 467, Loss: 0.4408, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 468, Loss: 0.4545, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 469, Loss: 0.4457, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 470, Loss: 0.4469, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 471, Loss: 0.4596, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 472, Loss: 0.4477, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 473, Loss: 0.4560, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 474, Loss: 0.4459, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 475, Loss: 0.4547, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 476, Loss: 0.4498, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 477, Loss: 0.4553, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 478, Loss: 0.4509, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 479, Loss: 0.4484, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 480, Loss: 0.4545, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 481, Loss: 0.4549, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 482, Loss: 0.4486, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 483, Loss: 0.4524, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 484, Loss: 0.4497, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 485, Loss: 0.4502, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 486, Loss: 0.4476, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 487, Loss: 0.4441, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 488, Loss: 0.4440, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 489, Loss: 0.4420, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 490, Loss: 0.4495, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 491, Loss: 0.4440, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 492, Loss: 0.4514, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 493, Loss: 0.4494, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 494, Loss: 0.4468, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 495, Loss: 0.4486, Val: 0.6897, Test: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 496, Loss: 0.4567, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 497, Loss: 0.4463, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 498, Loss: 0.4458, Val: 0.6897, Test: 1.0000\n",
      "Epoch: 499, Loss: 0.4493, Val: 0.6897, Test: 1.0000\n"
     ]
    }
   ],
   "source": [
    "best_val_perf = test_perf = 0\n",
    "for epoch in range(1, 500):\n",
    "    train_loss = train()\n",
    "    val_perf, tmp_test_perf = test()\n",
    "    if val_perf > best_val_perf:\n",
    "        best_val_perf = val_perf\n",
    "        test_perf = tmp_test_perf\n",
    "    log = 'Epoch: {:03d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    print(log.format(epoch, train_loss, best_val_perf, test_perf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02b177bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-e247fccb91f9>:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_link.loc[i]['to'] = f\n",
      "<ipython-input-2-e247fccb91f9>:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_link.loc[i]['from'] = t\n"
     ]
    }
   ],
   "source": [
    "e_sort_train = edge_sort_train(edge_train)\n",
    "e_sort_test = edge_sort_test(edge_test)\n",
    "y_train = e_sort_train.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7619d70e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5150e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_fea = pd.read_csv('con_100fea3.csv')\n",
    "x_train_con1 = get_fea1(e_sort_train, con_fea)\n",
    "x_test_con1 = get_fea1(e_sort_test, con_fea)\n",
    "\n",
    "x_train_con2 = get_fea2(e_sort_train, con_fea)\n",
    "x_test_con2 = get_fea2(e_sort_test, con_fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49a798f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "import lightgbm as lgbm\n",
    "\n",
    "lgb = lgbm.LGBMClassifier(random_state=0)\n",
    "lgb.fit(x_train_con1, y_train)\n",
    "\n",
    "ridge = RidgeClassifier()\n",
    "ridge.fit(x_train_con2.reshape(-1, 1), y_train)\n",
    "\n",
    "con1_result = lgb.predict_proba(x_test_con1)[:,1]\n",
    "\n",
    "d = ridge.decision_function(x_test_con2.reshape(-1, 1))\n",
    "d_2d = np.c_[-d, d]\n",
    "con2_result = softmax(d_2d)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39f95c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.encode()\n",
    "emb = pd.DataFrame(z.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bd57c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_emb1 = get_fea1(e_sort_train, emb)\n",
    "x_test_emb1 = get_fea1(e_sort_test, emb)\n",
    "\n",
    "x_train_emb2 = get_fea2(e_sort_train, emb)\n",
    "x_test_emb2 = get_fea2(e_sort_test, emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ce609da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb = lgbm.LGBMClassifier(random_state=0)\n",
    "lgb.fit(x_train_emb1, y_train)\n",
    "\n",
    "ridge = RidgeClassifier()\n",
    "ridge.fit(x_train_emb2.reshape(-1, 1), y_train)\n",
    "\n",
    "emb1_result = lgb.predict_proba(x_test_emb1)[:,1]\n",
    "\n",
    "d = ridge.decision_function(x_test_emb2.reshape(-1, 1))\n",
    "d_2d = np.c_[-d, d]\n",
    "emb2_result = softmax(d_2d)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8c3ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_edge = torch.tensor(e_sort_test.loc[:,['to', 'from']].values.T, dtype=torch.int64).cuda()\n",
    "logits = (z[test_edge[0]] * z[test_edge[1]]).sum(dim=-1)\n",
    "nn_result = logits.sigmoid().cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62bfa64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = np.mean([con1_result, con2_result, emb1_result, emb2_result, nn_result], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e7600ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'id': e_sort_test.id.values, 'prob': final}\n",
    "pred = pd.DataFrame(d)\n",
    "for i in u_ex.index:\n",
    "    u_ex.loc[i, 'prob'] = pred[pred.id == u_ex.loc[i, 'id']].prob.values\n",
    "    u_ex.to_csv('u3.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e93fc71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
