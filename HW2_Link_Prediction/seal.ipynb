{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42c4f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import networkx.convert_matrix\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.nn import SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cce23b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import os.path as osp\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.nn import ModuleList, Linear, Conv1d, MaxPool1d\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv, global_sort_pool\n",
    "from torch_geometric.data import Data, InMemoryDataset, DataLoader\n",
    "from torch_geometric.utils import (negative_sampling, add_self_loops,\n",
    "                                   train_test_split_edges, k_hop_subgraph,\n",
    "                                   to_scipy_sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74827503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre(train, content):\n",
    "    \n",
    "    \n",
    "    content.rename(columns={ content.columns[0]: \"id\" }, inplace=True)\n",
    "    contentArray = content.values\n",
    "    contentArray_sort = contentArray[contentArray[:,0].argsort()]\n",
    "    contentArray_sort = pd.DataFrame(contentArray_sort)\n",
    "\n",
    "    unconnected_pairs = []\n",
    "    matrix =np.zeros( (contentArray_sort.shape[0], contentArray_sort.shape[0]) )\n",
    "    \n",
    "    for index,row in train.iterrows():\n",
    "        if row[3]==1:\n",
    "            x = row[1]\n",
    "            y = row[2]\n",
    "            matrix[x, y] = 1\n",
    "            matrix[y, x] = 1\n",
    "        else:\n",
    "            x = row[1]\n",
    "            y = row[2]\n",
    "            matrix[x, y] = 0\n",
    "            matrix[y, x] = 0\n",
    "\n",
    "    return np.asmatrix(matrix), contentArray_sort\n",
    "def edge_sort_train(train):\n",
    "    train_link = train.loc[:,['to', 'from', 'label']]\n",
    "\n",
    "    for i in train_link.index:\n",
    "        t = train_link.loc[i]['to']\n",
    "        f = train_link.loc[i]['from']\n",
    "        if t > f :\n",
    "            train_link.loc[i]['to'] = f\n",
    "            train_link.loc[i]['from'] = t\n",
    "\n",
    "    train_link = train_link.sort_values(by='to')\n",
    "    train_link = train_link[~train_link.duplicated()] # -> with duplicated edge exists\n",
    "    train_link.index = range(train_link.shape[0])\n",
    "\n",
    "    return train_link\n",
    "\n",
    "def edge_sort_test(train):\n",
    "    train_link = train.loc[:,['id', 'to', 'from']]\n",
    "\n",
    "    for i in train_link.index:\n",
    "        t = train_link.loc[i]['to']\n",
    "        f = train_link.loc[i]['from']\n",
    "        if t > f :\n",
    "            train_link.loc[i]['to'] = f\n",
    "            train_link.loc[i]['from'] = t\n",
    "\n",
    "    train_link = train_link.sort_values(by='to')\n",
    "    train_link.index = range(train_link.shape[0])\n",
    "\n",
    "    return train_link\n",
    "\n",
    "def get_fea(e_sort, str_fea):\n",
    "    \n",
    "    fea = []\n",
    "    for i in e_sort.index:\n",
    "\n",
    "        t = str_fea.loc[e_sort.loc[i, 'to'], :].values\n",
    "        f = str_fea.loc[e_sort.loc[i, 'from'], :].values\n",
    "        fea.append(t * f)\n",
    "\n",
    "    fea = np.array(fea)\n",
    "    \n",
    "    return fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e9347c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26aabe0fad8d>:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_link.loc[i]['to'] = f\n",
      "<ipython-input-3-26aabe0fad8d>:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_link.loc[i]['from'] = t\n"
     ]
    }
   ],
   "source": [
    "edge_test = pd.read_csv('test_2.csv')\n",
    "e_sort_test = edge_sort_test(edge_test)\n",
    "u_ex = pd.read_csv('upload_2.csv')\n",
    "\n",
    "train = pd.read_csv('train_2.csv')\n",
    "edge_train = edge_sort_train(train)\n",
    "content = pd.read_csv('content_2.csv', delimiter='\\t',header=None)\n",
    "content = content.drop(0, axis=1)\n",
    "train_adj, content_p = pre(train, content)\n",
    "n_fea = pd.read_csv('n_100fea2.csv')\n",
    "\n",
    "# pd.read_csv('n_100fea2.csv')\n",
    "\n",
    "edge_c = np.concatenate((edge_train.loc[:, ['from', 'to']].values, edge_train.loc[:, ['to', 'from']].values), axis=0)\n",
    "edge_index = torch.tensor(edge_c, dtype=torch.long)\n",
    "\n",
    "d_t2 = Data(edge_index=edge_index.t())\n",
    "d_t2.num_nodes= content.shape[0]\n",
    "d_t2.x = torch.tensor(n_fea.values, dtype=torch.float32)\n",
    "# data = train_test_split_edges(data,  test_ratio=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d2f63a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d721a180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEALDataset(InMemoryDataset):\n",
    "    def __init__(self, dataset, num_hops, split='train'):\n",
    "        self.data = dataset\n",
    "        self.num_hops = num_hops\n",
    "        super(SEALDataset, self).__init__('data\\\\dt2')\n",
    "        index = ['train', 'val', 'test'].index(split)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[index])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['SEAL_train_data.pt', 'SEAL_val_data.pt', 'SEAL_test_data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        random.seed(12345)\n",
    "        torch.manual_seed(12345)\n",
    "\n",
    "        data = train_test_split_edges(self.data, 0.001)\n",
    "\n",
    "        edge_index, _ = add_self_loops(data.train_pos_edge_index)\n",
    "        data.train_neg_edge_index = negative_sampling(\n",
    "            edge_index, num_nodes=data.num_nodes,\n",
    "            num_neg_samples=data.train_pos_edge_index.size(1))\n",
    "\n",
    "        self.__max_z__ = 0\n",
    "\n",
    "        # Collect a list of subgraphs for training, validation and test.\n",
    "        train_pos_list = self.extract_enclosing_subgraphs(\n",
    "            data.train_pos_edge_index, data.train_pos_edge_index, 1)\n",
    "        train_neg_list = self.extract_enclosing_subgraphs(\n",
    "            data.train_neg_edge_index, data.train_pos_edge_index, 0)\n",
    "\n",
    "        val_pos_list = self.extract_enclosing_subgraphs(\n",
    "            data.val_pos_edge_index, data.train_pos_edge_index, 1)\n",
    "        val_neg_list = self.extract_enclosing_subgraphs(\n",
    "            data.val_neg_edge_index, data.train_pos_edge_index, 0)\n",
    "\n",
    "        test_edge = torch.tensor(e_sort_test.loc[:,['to', 'from']].values.T, dtype=torch.int64).cuda()\n",
    "        test_pos_list = self.extract_enclosing_subgraphs(\n",
    "            test_edge, data.train_pos_edge_index, 1)\n",
    "#         test_neg_list = self.extract_enclosing_subgraphs(\n",
    "#             data.test_neg_edge_index, data.train_pos_edge_index, 0)\n",
    "\n",
    "        # Convert labels to one-hot features.\n",
    "        for data in chain(train_pos_list, train_neg_list, val_pos_list,\n",
    "                          val_neg_list, test_pos_list):\n",
    "            data.x = F.one_hot(data.z, self.__max_z__ + 1).to(torch.float)\n",
    "\n",
    "        torch.save(self.collate(train_pos_list + train_neg_list),\n",
    "                   self.processed_paths[0])\n",
    "        torch.save(self.collate(val_pos_list + val_neg_list),\n",
    "                   self.processed_paths[1])\n",
    "        torch.save(self.collate(test_pos_list),\n",
    "                   self.processed_paths[2])\n",
    "\n",
    "    def extract_enclosing_subgraphs(self, link_index, edge_index, y):\n",
    "        data_list = []\n",
    "        for src, dst in link_index.t().tolist():\n",
    "            sub_nodes, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
    "                [src, dst], self.num_hops, edge_index, relabel_nodes=True)\n",
    "            src, dst = mapping.tolist()\n",
    "\n",
    "            # Remove target link from the subgraph.\n",
    "            mask1 = (sub_edge_index[0] != src) | (sub_edge_index[1] != dst)\n",
    "            mask2 = (sub_edge_index[0] != dst) | (sub_edge_index[1] != src)\n",
    "            sub_edge_index = sub_edge_index[:, mask1 & mask2]\n",
    "\n",
    "            # Calculate node labeling.\n",
    "            z = self.drnl_node_labeling(sub_edge_index, src, dst,\n",
    "                                        num_nodes=sub_nodes.size(0))\n",
    "\n",
    "            data = Data(x=self.data.x[sub_nodes], z=z,\n",
    "                        edge_index=sub_edge_index, y=y)\n",
    "            data_list.append(data)\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    def drnl_node_labeling(self, edge_index, src, dst, num_nodes=None):\n",
    "        # Double-radius node labeling (DRNL).\n",
    "        src, dst = (dst, src) if src > dst else (src, dst)\n",
    "        adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).tocsr()\n",
    "\n",
    "        idx = list(range(src)) + list(range(src + 1, adj.shape[0]))\n",
    "        adj_wo_src = adj[idx, :][:, idx]\n",
    "\n",
    "        idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))\n",
    "        adj_wo_dst = adj[idx, :][:, idx]\n",
    "\n",
    "        dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True,\n",
    "                                 indices=src)\n",
    "        dist2src = np.insert(dist2src, dst, 0, axis=0)\n",
    "        dist2src = torch.from_numpy(dist2src)\n",
    "\n",
    "        dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True,\n",
    "                                 indices=dst - 1)\n",
    "        dist2dst = np.insert(dist2dst, src, 0, axis=0)\n",
    "        dist2dst = torch.from_numpy(dist2dst)\n",
    "\n",
    "        dist = dist2src + dist2dst\n",
    "        dist_over_2, dist_mod_2 = dist // 2, dist % 2\n",
    "\n",
    "        z = 1 + torch.min(dist2src, dist2dst)\n",
    "        z += dist_over_2 * (dist_over_2 + dist_mod_2 - 1)\n",
    "        z[src] = 1.\n",
    "        z[dst] = 1.\n",
    "        z[torch.isnan(z)] = 0.\n",
    "\n",
    "        self.__max_z__ = max(int(z.max()), self.__max_z__)\n",
    "\n",
    "        return z.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9883823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset = Planetoid(root='data/Planetoid', name='Cora')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ed75fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "indices out of range 0...N",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a4fd341c1834>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSEALDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_t2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_hops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSEALDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_t2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_hops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSEALDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_t2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_hops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-86b62a97a528>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, num_hops, split)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_hops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_hops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSEALDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data\\\\dt2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'val'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessed_paths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\graph\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[0;32m     52\u001b[0m     def __init__(self, root=None, transform=None, pre_transform=None,\n\u001b[0;32m     53\u001b[0m                  pre_filter=None):\n\u001b[1;32m---> 54\u001b[1;33m         super(InMemoryDataset, self).__init__(root, transform, pre_transform,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                               pre_filter)\n\u001b[0;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\graph\\lib\\site-packages\\torch_geometric\\data\\dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'process'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\graph\\lib\\site-packages\\torch_geometric\\data\\dataset.py\u001b[0m in \u001b[0;36m_process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessed_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mosp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessed_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pre_transform.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-86b62a97a528>\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mtest_edge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me_sort_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'to'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'from'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         test_pos_list = self.extract_enclosing_subgraphs(\n\u001b[0m\u001b[0;32m     39\u001b[0m             test_edge, data.train_pos_edge_index, 1)\n\u001b[0;32m     40\u001b[0m \u001b[1;31m#         test_neg_list = self.extract_enclosing_subgraphs(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-86b62a97a528>\u001b[0m in \u001b[0;36mextract_enclosing_subgraphs\u001b[1;34m(self, link_index, edge_index, y)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;31m# Calculate node labeling.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             z = self.drnl_node_labeling(sub_edge_index, src, dst,\n\u001b[0m\u001b[0;32m     69\u001b[0m                                         num_nodes=sub_nodes.size(0))\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-86b62a97a528>\u001b[0m in \u001b[0;36mdrnl_node_labeling\u001b[1;34m(self, edge_index, src, dst, num_nodes)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0madj_wo_dst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True,\n\u001b[0m\u001b[0;32m     89\u001b[0m                                  indices=src)\n\u001b[0;32m     90\u001b[0m         \u001b[0mdist2src\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist2src\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_shortest_path.pyx\u001b[0m in \u001b[0;36mscipy.sparse.csgraph._shortest_path.shortest_path\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_shortest_path.pyx\u001b[0m in \u001b[0;36mscipy.sparse.csgraph._shortest_path.dijkstra\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: indices out of range 0...N"
     ]
    }
   ],
   "source": [
    "train_dataset = SEALDataset(d_t2, num_hops=2, split='train')\n",
    "val_dataset = SEALDataset(d_t2, num_hops=2, split='val')\n",
    "test_dataset = SEALDataset(d_t2, num_hops=2, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5961b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = SEALDataset(dataset, num_hops=2, split='train')\n",
    "# val_dataset = SEALDataset(dataset, num_hops=2, split='val')\n",
    "# test_dataset = SEALDataset(dataset, num_hops=2, split='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b737129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGCNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_layers, GNN=GCNConv, k=0.6):\n",
    "        super(DGCNN, self).__init__()\n",
    "\n",
    "        if k < 1:  # Transform percentile to number.\n",
    "            num_nodes = sorted([data.num_nodes for data in train_dataset])\n",
    "            k = num_nodes[int(math.ceil(k * len(num_nodes))) - 1]\n",
    "            k = max(10, k)\n",
    "        self.k = int(k)\n",
    "\n",
    "        self.convs = ModuleList()\n",
    "        self.convs.append(GNN(train_dataset.num_features, hidden_channels))\n",
    "        for i in range(0, num_layers - 1):\n",
    "            self.convs.append(GNN(hidden_channels, hidden_channels))\n",
    "        self.convs.append(GNN(hidden_channels, 1))\n",
    "\n",
    "        conv1d_channels = [16, 32]\n",
    "        total_latent_dim = hidden_channels * num_layers + 1\n",
    "        conv1d_kws = [total_latent_dim, 5]\n",
    "        self.conv1 = Conv1d(1, conv1d_channels[0], conv1d_kws[0],\n",
    "                            conv1d_kws[0])\n",
    "        self.maxpool1d = MaxPool1d(2, 2)\n",
    "        self.conv2 = Conv1d(conv1d_channels[0], conv1d_channels[1],\n",
    "                            conv1d_kws[1], 1)\n",
    "        dense_dim = int((self.k - 2) / 2 + 1)\n",
    "        dense_dim = (dense_dim - conv1d_kws[1] + 1) * conv1d_channels[1]\n",
    "        self.lin1 = Linear(dense_dim, 128)\n",
    "        self.lin2 = Linear(128, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        xs = [x]\n",
    "        for conv in self.convs:\n",
    "            xs += [torch.tanh(conv(xs[-1], edge_index))]\n",
    "        x = torch.cat(xs[1:], dim=-1)\n",
    "\n",
    "        # Global pooling.\n",
    "        x = global_sort_pool(x, batch, self.k)\n",
    "        x = x.unsqueeze(1)  # [num_graphs, 1, k * hidden]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool1d(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # [num_graphs, dense_dim]\n",
    "\n",
    "        # MLP.\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DGCNN(hidden_channels=32, num_layers=3).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(data.x, data.edge_index, data.batch)\n",
    "        loss = BCEWithLogitsLoss()(logits.view(-1), data.y.to(torch.float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "    return total_loss / len(train_dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    y_pred, y_true = [], []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        logits = model(data.x, data.edge_index, data.batch)\n",
    "        y_pred.append(logits.view(-1).cpu())\n",
    "        y_true.append(data.y.view(-1).cpu().to(torch.float))\n",
    "\n",
    "    return roc_auc_score(torch.cat(y_true), torch.cat(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b79b3d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.6928, Val: 0.5662, Test: 0.5639\n",
      "Epoch: 02, Loss: 0.6917, Val: 0.5683, Test: 0.5657\n",
      "Epoch: 03, Loss: 0.6889, Val: 0.5711, Test: 0.5683\n",
      "Epoch: 04, Loss: 0.6848, Val: 0.5752, Test: 0.5708\n",
      "Epoch: 05, Loss: 0.6793, Val: 0.5838, Test: 0.5741\n",
      "Epoch: 06, Loss: 0.6754, Val: 0.5884, Test: 0.5811\n",
      "Epoch: 07, Loss: 0.6720, Val: 0.5941, Test: 0.5914\n",
      "Epoch: 08, Loss: 0.6697, Val: 0.6020, Test: 0.5971\n",
      "Epoch: 09, Loss: 0.6675, Val: 0.5952, Test: 0.5971\n",
      "Epoch: 10, Loss: 0.6666, Val: 0.6005, Test: 0.5971\n",
      "Epoch: 11, Loss: 0.6655, Val: 0.6001, Test: 0.5971\n",
      "Epoch: 12, Loss: 0.6647, Val: 0.6039, Test: 0.6000\n",
      "Epoch: 13, Loss: 0.6635, Val: 0.5926, Test: 0.6000\n",
      "Epoch: 14, Loss: 0.6630, Val: 0.6032, Test: 0.6000\n",
      "Epoch: 15, Loss: 0.6619, Val: 0.6045, Test: 0.6059\n",
      "Epoch: 16, Loss: 0.6617, Val: 0.5967, Test: 0.6059\n",
      "Epoch: 17, Loss: 0.6601, Val: 0.5944, Test: 0.6059\n",
      "Epoch: 18, Loss: 0.6594, Val: 0.5932, Test: 0.6059\n",
      "Epoch: 19, Loss: 0.6593, Val: 0.5956, Test: 0.6059\n",
      "Epoch: 20, Loss: 0.6585, Val: 0.6032, Test: 0.6059\n",
      "Epoch: 21, Loss: 0.6577, Val: 0.5969, Test: 0.6059\n",
      "Epoch: 22, Loss: 0.6563, Val: 0.6031, Test: 0.6059\n",
      "Epoch: 23, Loss: 0.6562, Val: 0.6041, Test: 0.6059\n",
      "Epoch: 24, Loss: 0.6555, Val: 0.6060, Test: 0.6169\n",
      "Epoch: 25, Loss: 0.6552, Val: 0.6078, Test: 0.6280\n",
      "Epoch: 26, Loss: 0.6543, Val: 0.6087, Test: 0.6264\n",
      "Epoch: 27, Loss: 0.6540, Val: 0.6092, Test: 0.6281\n",
      "Epoch: 28, Loss: 0.6534, Val: 0.6089, Test: 0.6281\n",
      "Epoch: 29, Loss: 0.6520, Val: 0.6112, Test: 0.6251\n",
      "Epoch: 30, Loss: 0.6517, Val: 0.6115, Test: 0.6220\n",
      "Epoch: 31, Loss: 0.6514, Val: 0.6102, Test: 0.6220\n",
      "Epoch: 32, Loss: 0.6507, Val: 0.6099, Test: 0.6220\n",
      "Epoch: 33, Loss: 0.6495, Val: 0.6067, Test: 0.6220\n",
      "Epoch: 34, Loss: 0.6502, Val: 0.6092, Test: 0.6220\n",
      "Epoch: 35, Loss: 0.6485, Val: 0.6093, Test: 0.6220\n",
      "Epoch: 36, Loss: 0.6486, Val: 0.6124, Test: 0.6328\n",
      "Epoch: 37, Loss: 0.6475, Val: 0.6111, Test: 0.6328\n",
      "Epoch: 38, Loss: 0.6463, Val: 0.6116, Test: 0.6328\n",
      "Epoch: 39, Loss: 0.6463, Val: 0.6123, Test: 0.6328\n",
      "Epoch: 40, Loss: 0.6464, Val: 0.6130, Test: 0.6371\n",
      "Epoch: 41, Loss: 0.6452, Val: 0.6144, Test: 0.6342\n",
      "Epoch: 42, Loss: 0.6447, Val: 0.6142, Test: 0.6342\n",
      "Epoch: 43, Loss: 0.6447, Val: 0.6115, Test: 0.6342\n",
      "Epoch: 44, Loss: 0.6440, Val: 0.6146, Test: 0.6369\n",
      "Epoch: 45, Loss: 0.6452, Val: 0.6148, Test: 0.6333\n",
      "Epoch: 46, Loss: 0.6440, Val: 0.6117, Test: 0.6333\n",
      "Epoch: 47, Loss: 0.6431, Val: 0.6151, Test: 0.6271\n",
      "Epoch: 48, Loss: 0.6427, Val: 0.6170, Test: 0.6345\n",
      "Epoch: 49, Loss: 0.6423, Val: 0.6138, Test: 0.6345\n",
      "Epoch: 50, Loss: 0.6420, Val: 0.6157, Test: 0.6345\n"
     ]
    }
   ],
   "source": [
    "best_val_auc = test_auc = 0\n",
    "for epoch in range(1, 51):\n",
    "    loss = train()\n",
    "    val_auc = test(val_loader)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        test_auc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
